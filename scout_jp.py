import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
from datetime import datetime
import time


class MacroScouter:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        # í•„í„° í‚¤ì›Œë“œ
        self.jp_keywords = [
            # --- ì£¼ìš” ê¸°ê´€ ë° ì •ì±… ---
            'æ—¥éŠ€',          # ì¼ë³¸ì€í–‰ (BOJ)
            'è²¡å‹™çœ',        # ì¬ë¬´ì„± (ê¸°ì¬ë¶€ ì—­í• )
            'é‡‘èæ”¿ç­–',      # ê¸ˆìœµì •ì±…
            'é‡‘åˆ©',          # ê¸ˆë¦¬
            'ç‚ºæ›¿',          # í™˜ìœ¨
            'ç‰©ä¾¡',          # ë¬¼ê°€
            'å††',            # ì—”í™”
            'ä»‹å…¥',          # ê°œì… (ì™¸í™˜ì‹œì¥ ê°œì…)
            'æ±ºå®šä¼šåˆ',      # ì •ì±…ê²°ì •íšŒì˜
            'ç·©å’Œ',          # ì™„í™” (ëˆ í’€ê¸°)
            'å‡ºå£',          # ì¶œêµ¬ (ê¸´ì¶• ì „í™˜)

            # --- í•µì‹¬ ì§ì±… (ëˆ„ê°€ ì•‰ì•„ë„ ì¡íˆëŠ” ê·¸ë¬¼) ---
            'ç·è£',          # ì´ì¬ (BOJ ìˆ˜ì¥)
            'å‰¯ç·è£',        # ë¶€ì´ì¬
            'å¯©è­°å§”å“¡',      # ì‹¬ì˜ìœ„ì› (BOJ ê¸ˆí†µìœ„ì›)
            'è²¡å‹™ç›¸',        # ì¬ë¬´ìƒ (ì¬ë¬´ë¶€ ì¥ê´€)
            'è²¡å‹™å®˜',        # ì¬ë¬´ê´€ (ì‹¤ë¬´ ì±…ì„ì)
            'é¦–ç›¸',          # ìˆ˜ìƒ (ì´ë¦¬)
            'é–£åƒš',          # ê°ë£Œ (ì¥ê´€ê¸‰ ì¸ì‚¬)

            # --- í•µì‹¬ ê²½ì œ ì§€í‘œ (ì‹ ê·œ ì¶”ê°€) ---
            'GDP',          # êµ­ë‚´ì´ìƒì‚°
            'CPI',          # ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜
            'çŸ­è¦³',          # ë‹¨ì¹¸ì§€í‘œ (ê¸°ì—…ê²½ê¸°ì‹¤ì‚¬ì§€í‘œ)
            'è²¿æ˜“åæ”¯',      # ë¬´ì—­ìˆ˜ì§€ (ìˆ˜ì¶œì… ìƒí™©)
            'å¤±æ¥­ç‡',        # ì‹¤ì—…ë¥ 
            'è³ƒé‡‘',          # ì„ê¸ˆ (BOJê°€ ìµœê·¼ ê°€ì¥ ê°•ì¡°í•¨)
            'å®Ÿè³ªè³ƒé‡‘',      # ì‹¤ì§ˆì„ê¸ˆ (ì´ê²Œ ì˜¬ë¼ì•¼ ê¸ˆë¦¬ ì˜¬ë¦¼)
            'å®¶è¨ˆèª¿æŸ»',      # ê°€ê³„ì¡°ì‚¬ (ì†Œë¹„ì ì§€ì¶œ)
            'æ©Ÿæ¢°å—æ³¨',      # ê¸°ê³„ìˆ˜ì£¼ (ê¸°ì—… íˆ¬ì ì§€í‘œ)
            'æ™¯æ°—å‹•å‘æŒ‡æ•°',   # ê²½ê¸°ë™í–¥ì§€ìˆ˜

            # --- í•µì‹¬ ì•¡ì…˜ ë° ìƒíƒœ ---
            'åˆ©ä¸Šã’',        # ê¸ˆë¦¬ ì¸ìƒ (ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´!)
            'åˆ©ä¸‹ã’',        # ê¸ˆë¦¬ ì¸í•˜
            'æ®ãˆç½®ã',      # ê¸ˆë¦¬ ë™ê²°
            'ä¿®æ­£',          # ì •ì±… ìˆ˜ì •
            'ç‚¹æ¤œ',          # ì •ì±… ì ê²€
            'å½“é¢',          # ë‹¹ë¶„ê°„ (ì •ì±… ìœ ì§€ ì‹œ ìì£¼ ì”€)
            'è¸ã¿è¾¼ã‚€'       # (ì •ì±…ì—) ë°œì„ ë“¤ì´ë‹¤, ë‹¨í–‰í•˜ë‹¤
        ]
        self.db_path = "macro_intelligence.db"
        self._init_db()

    def _init_db(self):
        """DB ì´ˆê¸°í™” (country ì»¬ëŸ¼ í¬í•¨)"""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute('''CREATE TABLE IF NOT EXISTS news 
                       (link TEXT PRIMARY KEY, title TEXT, source TEXT, 
                        country TEXT, date TEXT, collected_at TEXT)''')
        conn.commit()
        conn.close()

    def _is_new(self, link):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute("SELECT link FROM news WHERE link=?", (link,))
        exists = cur.fetchone()
        conn.close()
        return exists is None

    def _save_to_db(self, item, country="Japan"):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        try:
            cur.execute("INSERT INTO news (link, title, source, country, date, collected_at) VALUES (?, ?, ?, ?, ?, ?)",
                        (item['link'], item['title'], item['source'], country, item['date'], datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            conn.commit()
        except sqlite3.IntegrityError:
            pass
        finally:
            conn.close()

    def _get_soup(self, url):
        try:
            res = requests.get(url, headers=self.headers, timeout=15)
            res.encoding = 'utf-8'
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"Connection Error ({url}): {e}")
        return None

    def scout_boj_universal(self, year=None):
        """[ì„±ê³µí–ˆë˜ ë¡œì§] ë§í¬ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ BOJ ìë£Œë¥¼ ë‚šì•„ì±•ë‹ˆë‹¤."""
        if year:
            url = f"https://www.boj.or.jp/about/press/koen_{year}/index.htm"
            source_name = f"BOJ_{year}"
        else:
            url = "https://www.boj.or.jp/about/press/index.htm"
            source_name = "BOJ_Latest"

        soup = self._get_soup(url)
        results = []
        if not soup:
            return results

        # ì•„ê¹Œ ì„±ê³µí–ˆë˜ 'ëª¨ë“  ë§í¬ ë’¤ì§€ê¸°' ì „ëµ
        links = soup.find_all('a', href=True)
        for a in links:
            href = a['href']
            title = a.get_text().strip()

            # BOJ ê°•ì—°ë¬¸/ë³´ë„ìë£Œ íŠ¹ìœ ì˜ ë§í¬ íŒ¨í„´ (/ko ë˜ëŠ” koen)
            if ('/ko' in href or 'koen' in href) and len(title) > 10:
                full_link = "https://www.boj.or.jp" + \
                    href if href.startswith('/') else href

                # ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                date = f"{year}-XX" if year else "Latest"
                try:
                    # í‘œ êµ¬ì¡°ì¸ ê²½ìš° ì• ì¹¸ì˜ ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
                    date = a.find_parent('tr').find('td').get_text().strip()
                except:
                    pass

                results.append({
                    "source": source_name,
                    "date": date,
                    "title": title,
                    "link": full_link
                })
        return results

    def scout_news_jp(self):
        """ì§€êµ¬í†µì‹  ë° ë¡œì´í„° ì •ì°°"""
        sources = [
            {"name": "Jiji", "url": "https://www.jiji.com/jc/c?g=eco"},
            {"name": "Reuters", "url": "https://jp.reuters.com/markets/japan/"}
        ]
        results = []
        for src in sources:
            soup = self._get_soup(src['url'])
            if not soup:
                continue

            links = soup.find_all('a', href=True)
            for link in links:
                title = link.get_text().strip()
                # í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
                if len(title) > 15 and any(kw in title for kw in self.jp_keywords):
                    href = link['href']
                    full_link = href if href.startswith('http') else (
                        "https://www.jiji.com" +
                        href if src['name'] == "Jiji" else "https://jp.reuters.com" + href
                    )
                    results.append({
                        "source": src['name'], "date": "Today", "title": title, "link": full_link
                    })
        return results

    def run_all_jp(self):
        """ì¼ë³¸ ì •ì°° í†µí•© ì‹¤í–‰"""
        print("ğŸ›°ï¸ ì¼ë³¸ ë§¤í¬ë¡œ í†µí•© ì •ì°° ê°œì‹œ...")
        new_items = []

        # 1. BOJ ìµœì‹ 
        for item in self.scout_boj_universal():
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                new_items.append(item)

        # 2. ë‰´ìŠ¤ (ì§€ì, ë¡œì´í„°)
        for item in self.scout_news_jp():
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                new_items.append(item)
        return new_items


if __name__ == "__main__":
    scouter = MacroScouter()

    # [1] ê³¼ê±° ë°ì´í„° ì±„ìš°ê¸° (2021~2025)
    print("â³ ê³¼ê±° ì•„ì¹´ì´ë¸Œ ì±„ìš°ëŠ” ì¤‘...")
    for y in range(2021, 2026):
        historical = scouter.scout_boj_universal(year=y)
        count = 0
        for item in historical:
            if scouter._is_new(item['link']):
                scouter._save_to_db(item, country="Japan")
                count += 1
        print(f"âœ… {y}ë…„ë„ ì™„ë£Œ: {count}ê±´ ì €ì¥")

    # [2] ì˜¤ëŠ˜ì ìµœì‹  ì •ì°°
    new_data = scouter.run_all_jp()
    if new_data:
        print(f"ğŸ”¥ ì‹ ê·œ ë°ì´í„° {len(new_data)}ê±´ ë°œê²¬!")
    else:
        print("â„¹ï¸ ìƒˆë¡œ ì—…ë°ì´íŠ¸ëœ ì†Œì‹ ì—†ìŒ.")
