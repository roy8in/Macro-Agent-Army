import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
from datetime import datetime
import time


class MacroScouter:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        # í•„í„° í‚¤ì›Œë“œ
        self.jp_keywords = [
            # --- ì£¼ìš” ê¸°ê´€ ë° ì •ì±… ---
            'æ—¥éŠ€',          # ì¼ë³¸ì€í–‰ (BOJ)
            'è²¡å‹™çœ',        # ì¬ë¬´ì„± (ê¸°ì¬ë¶€ ì—­í• )
            'é‡‘èæ”¿ç­–',      # ê¸ˆìœµì •ì±…
            'é‡‘åˆ©',          # ê¸ˆë¦¬
            'ç‚ºæ›¿',          # í™˜ìœ¨
            'ç‰©ä¾¡',          # ë¬¼ê°€
            'å††',            # ì—”í™”
            'ä»‹å…¥',          # ê°œì… (ì™¸í™˜ì‹œì¥ ê°œì…)
            'æ±ºå®šä¼šåˆ',      # ì •ì±…ê²°ì •íšŒì˜
            'ç·©å’Œ',          # ì™„í™” (ëˆ í’€ê¸°)
            'å‡ºå£',          # ì¶œêµ¬ (ê¸´ì¶• ì „í™˜)

            # --- í•µì‹¬ ì§ì±… (ëˆ„ê°€ ì•‰ì•„ë„ ì¡íˆëŠ” ê·¸ë¬¼) ---
            'ç·è£',          # ì´ì¬ (BOJ ìˆ˜ì¥)
            'å‰¯ç·è£',        # ë¶€ì´ì¬
            'å¯©è­°å§”å“¡',      # ì‹¬ì˜ìœ„ì› (BOJ ê¸ˆí†µìœ„ì›)
            'è²¡å‹™ç›¸',        # ì¬ë¬´ìƒ (ì¬ë¬´ë¶€ ì¥ê´€)
            'è²¡å‹™å®˜',        # ì¬ë¬´ê´€ (ì‹¤ë¬´ ì±…ì„ì)
            'é¦–ç›¸',          # ìˆ˜ìƒ (ì´ë¦¬)
            'é–£åƒš',          # ê°ë£Œ (ì¥ê´€ê¸‰ ì¸ì‚¬)

            # --- í•µì‹¬ ê²½ì œ ì§€í‘œ (ì‹ ê·œ ì¶”ê°€) ---
            'GDP',          # êµ­ë‚´ì´ìƒì‚°
            'CPI',          # ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜
            'çŸ­è¦³',          # ë‹¨ì¹¸ì§€í‘œ (ê¸°ì—…ê²½ê¸°ì‹¤ì‚¬ì§€í‘œ)
            'è²¿æ˜“åæ”¯',      # ë¬´ì—­ìˆ˜ì§€ (ìˆ˜ì¶œì… ìƒí™©)
            'å¤±æ¥­ç‡',        # ì‹¤ì—…ë¥ 
            'è³ƒé‡‘',          # ì„ê¸ˆ (BOJê°€ ìµœê·¼ ê°€ì¥ ê°•ì¡°í•¨)
            'å®Ÿè³ªè³ƒé‡‘',      # ì‹¤ì§ˆì„ê¸ˆ (ì´ê²Œ ì˜¬ë¼ì•¼ ê¸ˆë¦¬ ì˜¬ë¦¼)
            'å®¶è¨ˆèª¿æŸ»',      # ê°€ê³„ì¡°ì‚¬ (ì†Œë¹„ì ì§€ì¶œ)
            'æ©Ÿæ¢°å—æ³¨',      # ê¸°ê³„ìˆ˜ì£¼ (ê¸°ì—… íˆ¬ì ì§€í‘œ)
            'æ™¯æ°—å‹•å‘æŒ‡æ•°',   # ê²½ê¸°ë™í–¥ì§€ìˆ˜

            # --- í•µì‹¬ ì•¡ì…˜ ë° ìƒíƒœ ---
            'åˆ©ä¸Šã’',        # ê¸ˆë¦¬ ì¸ìƒ (ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´!)
            'åˆ©ä¸‹ã’',        # ê¸ˆë¦¬ ì¸í•˜
            'æ®ãˆç½®ã',      # ê¸ˆë¦¬ ë™ê²°
            'ä¿®æ­£',          # ì •ì±… ìˆ˜ì •
            'ç‚¹æ¤œ',          # ì •ì±… ì ê²€
            'å½“é¢',          # ë‹¹ë¶„ê°„ (ì •ì±… ìœ ì§€ ì‹œ ìì£¼ ì”€)
            'è¸ã¿è¾¼ã‚€'       # (ì •ì±…ì—) ë°œì„ ë“¤ì´ë‹¤, ë‹¨í–‰í•˜ë‹¤
        ]
        self.db_path = "macro_intelligence.db"
        self._init_db()

    def _init_db(self):
        """DB ì´ˆê¸°í™” (country ì»¬ëŸ¼ í¬í•¨)"""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute('''CREATE TABLE IF NOT EXISTS news 
                       (link TEXT PRIMARY KEY, title TEXT, source TEXT, 
                        country TEXT, date TEXT, collected_at TEXT)''')
        conn.commit()
        conn.close()

    def _is_new(self, link):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute("SELECT link FROM news WHERE link=?", (link,))
        exists = cur.fetchone()
        conn.close()
        return exists is None

    def _save_to_db(self, item, country="Japan"):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        try:
            cur.execute("INSERT INTO news (link, title, source, country, date, collected_at) VALUES (?, ?, ?, ?, ?, ?)",
                        (item['link'], item['title'], item['source'], country, item['date'], datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            conn.commit()
        except sqlite3.IntegrityError:
            pass
        finally:
            conn.close()

    def _get_soup(self, url):
        try:
            res = requests.get(url, headers=self.headers, timeout=15)
            res.encoding = 'utf-8'
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"Connection Error ({url}): {e}")
        return None

    def scout_boj_universal(self, year=None):
        """së§í¬ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ BOJ ìë£Œë¥¼ ë‚šì•„ì±•ë‹ˆë‹¤."""
        if year:
            url = f"https://www.boj.or.jp/about/press/koen_{year}/index.htm"
            source_name = f"BOJ_{year}"
        else:
            url = "https://www.boj.or.jp/about/press/index.htm"
            source_name = "BOJ_Latest"

        soup = self._get_soup(url)
        results = []
        if not soup:
            return results

        # 'ëª¨ë“  ë§í¬ ë’¤ì§€ê¸°' ì „ëµ
        links = soup.find_all('a', href=True)
        for a in links:
            href = a['href']
            title = a.get_text().strip()

            if ('/ko' in href or 'koen' in href) and len(title) > 10:
                full_link = "https://www.boj.or.jp" + \
                    href if href.startswith('/') else href

                # 1. ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                found_date = None
                try:
                    # ì‹¤ì œ í‘œ(tr) ì•ˆì— ë‚ ì§œ(td)ê°€ ìˆëŠ”ì§€ í™•ì¸
                    found_date = a.find_parent('tr').find(
                        'td').get_text().strip()
                except:
                    # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ê¸°ì‚¬ê°€ ì•„ë‹ í™•ë¥ ì´ ë†’ìœ¼ë¯€ë¡œ 'found_date'ëŠ” None
                    pass

                # ğŸš¨ [í•µì‹¬ ìˆ˜ì •]
                # ë‚ ì§œë¥¼ ëª» ì°¾ì•˜ê±°ë‚˜, ì°¾ì•˜ëŠ”ë° 'latest' ê°™ì€ ì•ˆë‚´ ë¬¸êµ¬ë¼ë©´ ì €ì¥í•˜ì§€ ì•Šê³  ê±´ë„ˆëœ€
                if not found_date or "latest" in found_date.lower():
                    continue

                # ì •ìƒì ì¸ ë‚ ì§œê°€ ìˆì„ ë•Œë§Œ ì§„í–‰
                date = self._standardize_date(found_date, year_hint=year)

                results.append({
                    "source": source_name,
                    "date": date,
                    "title": title,
                    "link": full_link
                })
        return results

    def scout_news_jp(self):
        """ì‹œì‚¬í†µì‹  ë° ë¡œì´í„° ì •ì°°"""
        sources = [
            {"name": "Jiji", "url": "https://www.jiji.com/jc/c?g=eco"},
            {"name": "Reuters", "url": "https://jp.reuters.com/markets/japan/"}
        ]
        results = []
        for src in sources:
            soup = self._get_soup(src['url'])
            if not soup:
                continue

            links = soup.find_all('a', href=True)
            for link in links:
                title = link.get_text().strip()
                # í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
                if len(title) > 15 and any(kw in title for kw in self.jp_keywords):
                    href = link['href']
                    full_link = href if href.startswith('http') else (
                        "https://www.jiji.com" +
                        href if src['name'] == "Jiji" else "https://jp.reuters.com" + href
                    )
                    results.append({
                        "source": src['name'],
                        # ì˜¤ëŠ˜ ë‚ ì§œ(YYYY-MM-DD)ë¡œ ë³€í™˜
                        "date": self._standardize_date("Today"),
                        "title": title,
                        "link": full_link
                    })
        return results

    def _standardize_date(self, date_str, year_hint=None):
        """ì¤‘êµ¬ë‚œë°©ì¸ ë‚ ì§œ í˜•ì‹ì„ YYYY-MM-DDë¡œ í†µì¼í•©ë‹ˆë‹¤."""
        now = datetime.now()

        # 1. 'Today', 'Recent' ì²˜ë¦¬ -> ì˜¤ëŠ˜ ë‚ ì§œë¡œ
        if any(word in date_str.lower() for word in ['today', 'recent', 'æœ€æ–°']):
            return now.strftime('%Y-%m-%d')

        # 2. '2025å¹´ 7æœˆ 3æ—¥' ì²˜ë¦¬
        if 'å¹´' in date_str:
            date_str = date_str.replace(
                'å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace(' ', '')
            # 2025-7-3 -> 2025-07-03 í˜•íƒœë¡œ ë³´ì •
            try:
                dt = datetime.strptime(date_str, '%Y-%m-%d')
                return dt.strftime('%Y-%m-%d')
            except:
                pass

        # 3. '2021-XX' ì²˜ë¦¬ (ê³¼ê±° ë°ì´í„° ë°±í•„ìš©)
        if '-XX' in date_str:
            return f"{date_str.split('-')[0]}-01-01"  # ì¼ë‹¨ ê·¸ í•´ 1ì›” 1ì¼ë¡œ ì²˜ë¦¬

        # 4. ê¸°íƒ€ '07/03' ë“± ì—°ë„ê°€ ì—†ëŠ” ê²½ìš°
        # (BOJ ìµœì‹  ë¦¬ìŠ¤íŠ¸ ë“±ì—ì„œ ì—°ë„ ì—†ì´ ì›”/ì¼ë§Œ ë‚˜ì˜¬ ë•Œ ì‚¬ìš©)
        if len(date_str) <= 6 and '/' in date_str:
            year = year_hint if year_hint else now.year
            return f"{year}-{date_str.replace('/', '-')}"

        return date_str  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€

    def run_all_jp(self):
        """ì¼ë³¸ ì •ì°° í†µí•© ì‹¤í–‰"""
        print("ğŸ›°ï¸ ì¼ë³¸ ë§¤í¬ë¡œ í†µí•© ì •ì°° ê°œì‹œ...")
        new_items = []

        # 1. BOJ ìµœì‹ 
        for item in self.scout_boj_universal():
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                new_items.append(item)

        # 2. ë‰´ìŠ¤ (ì§€ì, ë¡œì´í„°)
        for item in self.scout_news_jp():
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                new_items.append(item)
        return new_items


if __name__ == "__main__":
    scouter = MacroScouter()

    # [1] ê³¼ê±° ë°ì´í„° ì±„ìš°ê¸° (2021~2025)
    # print("â³ ê³¼ê±° ì•„ì¹´ì´ë¸Œ ì±„ìš°ëŠ” ì¤‘...")
    # for y in range(2021, 2026):
    #     historical = scouter.scout_boj_universal(year=y)
    #     count = 0
    #     for item in historical:
    #         if scouter._is_new(item['link']):
    #             scouter._save_to_db(item, country="Japan")
    #             count += 1
    #     print(f"âœ… {y}ë…„ë„ ì™„ë£Œ: {count}ê±´ ì €ì¥")

    # [2] ì˜¤ëŠ˜ì ìµœì‹  ì •ì°°
    new_data = scouter.run_all_jp()
    if new_data:
        print(f"ğŸ”¥ ì‹ ê·œ ë°ì´í„° {len(new_data)}ê±´ ë°œê²¬!")
    else:
        print("â„¹ï¸ ìƒˆë¡œ ì—…ë°ì´íŠ¸ëœ ì†Œì‹ ì—†ìŒ.")
