import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime
import urllib3
from dateutil import parser  # ğŸ†• ì§€ëŠ¥í˜• ë‚ ì§œ íŒŒì‹±ì„ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroScouter:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        # [ê¸°ì¡´ ì¼ë³¸ í‚¤ì›Œë“œ]
        self.jp_keywords = ['æ—¥éŠ€', 'é‡‘èæ”¿ç­–', 'é‡‘åˆ©', 'å††', 'åˆ©ä¸Šã’', 'é¦–ç›¸', 'GDP', 'CPI'] 
        # [ì‹ ê·œ ë¯¸êµ­/ê¸€ë¡œë²Œ í‚¤ì›Œë“œ]
        self.en_keywords = ['Fed', 'Rate', 'Inflation', 'CPI', 'Treasury', 'Powell', 'Nvidia', 'Stocks', 'Dollar']
        
        self.db_path = "macro_intelligence.db"
        self._init_db()

    def _init_db(self):
        """DB ë° í…Œì´ë¸” ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute('''CREATE TABLE IF NOT EXISTS news 
                       (link TEXT PRIMARY KEY, title TEXT, source TEXT, 
                        country TEXT, date TEXT, collected_at TEXT)''')
        conn.commit()
        conn.close()

    def _standardize_date(self, date_str):
        """ ì§€ì €ë¶„í•œ ë‚ ì§œ í˜•ì‹ì„ 'YYYY-MM-DD HH:MM:SS'ë¡œ ì„¸íƒí•©ë‹ˆë‹¤. """
        if not date_str:
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        try:
            # ISO 8601, RFC 822 ë“± ë‹¤ì–‘í•œ í˜•ì‹ì„ ìë™ìœ¼ë¡œ ì½ì–´ëƒ…ë‹ˆë‹¤.
            dt = parser.parse(date_str)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ì„¸ì²™ (T, Z ì œê±°)
            clean = date_str.replace('T', ' ').replace('Z', '')
            return clean[:19]

    def _is_new(self, link):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute("SELECT link FROM news WHERE link=?", (link,))
        exists = cur.fetchone()
        conn.close()
        return exists is None

    def _save_to_db(self, item, country):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        try:
            cur.execute("INSERT OR IGNORE INTO news (link, title, source, country, date, collected_at) VALUES (?, ?, ?, ?, ?, ?)",
                        (item['link'], item['title'], item['source'], country, item['date'], datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            conn.commit()
        except sqlite3.IntegrityError:
            pass
        finally:
            conn.close()

    def _get_soup(self, url, is_xml=False):
        try:
            res = requests.get(url, headers=self.headers, verify=False, timeout=15)
            if res.status_code == 200:
                features = "xml" if is_xml else "html.parser"
                return BeautifulSoup(res.content, features)
        except Exception as e:
            print(f"ğŸ“¡ ì ‘ì† ì—ëŸ¬ ({url}): {e}")
        return None

    def scout_yahoo(self):
        """ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ ì •ì°° ë° ë‚ ì§œ ì •ê·œí™”"""
        sources = [
            {"name": "Yahoo_Index", "url": "https://finance.yahoo.com/news/rssindex"},
            {"name": "Yahoo_Stocks", "url": "https://finance.yahoo.com/rss/stocks"}
        ]
        results = []
        for src in sources:
            soup = self._get_soup(src['url'], is_xml=True)
            if not soup: continue

            items = soup.find_all('item')
            for item in items:
                title = item.title.text.strip()
                link = item.link.text.split('?')[0]
                pub_date = item.pubDate.text if item.pubDate else ""
                
                # ğŸš¨ ë‚ ì§œ ì„¸íƒê¸° ì ìš©
                standard_date = self._standardize_date(pub_date)
                
                results.append({
                    "source": src['name'],
                    "date": standard_date,
                    "title": title,
                    "link": link
                })
        return results

    def scout_news_jp(self):
        """ì¼ë³¸ ë‰´ìŠ¤ ì •ì°° ë° ë‚ ì§œ ì •ê·œí™”"""
        sources = [
            {"name": "Jiji", "url": "https://www.jiji.com/jc/c?g=eco"},
            {"name": "Reuters_JP", "url": "https://jp.reuters.com/markets/japan/"}
        ]
        results = []
        for src in sources:
            soup = self._get_soup(src['url'])
            if not soup: continue
            links = soup.find_all('a', href=True)
            for link in links:
                title = link.get_text().strip()
                if len(title) > 15 and any(kw in title for kw in self.jp_keywords):
                    href = link['href']
                    full_link = href if href.startswith('http') else (
                        "https://www.jiji.com" + href if src['name'] == "Jiji" else "https://jp.reuters.com" + href
                    )
                    # ì¼ë³¸ ë‰´ìŠ¤ëŠ” ë³´í†µ ìˆ˜ì§‘ ì‹œì ì´ ë°œìƒ ì‹œì ê³¼ ìœ ì‚¬í•˜ë¯€ë¡œ í˜„ì¬ ì‹œê°„ ì ìš©
                    results.append({
                        "source": src['name'],
                        "date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "title": title,
                        "link": full_link
                    })
        return results

    def run_all_stations(self):
        """ì „ ì§€ì—­ í†µí•© ì •ì°° ì‹¤í–‰"""
        print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] ì „ ì§€ì—­ í†µí•© ì •ì°° ê°œì‹œ...")
        
        # 1. ë¯¸êµ­/ê¸€ë¡œë²Œ (Yahoo)
        yahoo_items = self.scout_yahoo()
        y_count = 0
        for item in yahoo_items:
            if self._is_new(item['link']):
                self._save_to_db(item, country="USA/Global")
                y_count += 1
        print(f"ğŸ‡ºğŸ‡¸ ì•¼í›„ íŒŒì´ë‚¸ìŠ¤: {y_count}ê±´ ì‹ ê·œ í™•ë³´")

        # 2. ì¼ë³¸ (Jiji, Reuters)
        jp_items = self.scout_news_jp()
        j_count = 0
        for item in jp_items:
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                j_count += 1
        print(f"ğŸ‡¯ğŸ‡µ ì¼ë³¸ ì‹œì¥: {j_count}ê±´ ì‹ ê·œ í™•ë³´")

if __name__ == "__main__":
    scouter = MacroScouter()
    scouter.run_all_stations()
