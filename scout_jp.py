import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime
import urllib3
from dateutil import parser # pip install python-dateutil í•„ìˆ˜

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class MacroScouter:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # ğŸ‡¯ğŸ‡µ [ì¼ë³¸ ì •ë°€ íƒ€ê²© í‚¤ì›Œë“œ - ì‚¬ë ¹ê´€ë‹˜ì˜ ì›ë³¸ ì£¼ì„ 100% ë³µêµ¬]
        self.jp_keywords = [
            # --- ì£¼ìš” ê¸°ê´€ ë° ì •ì±… ---
            'æ—¥éŠ€',          # ì¼ë³¸ì€í–‰ (BOJ)
            'è²¡å‹™çœ',        # ì¬ë¬´ì„± (ê¸°ì¬ë¶€ ì—­í• )
            'é‡‘èæ”¿ç­–',      # ê¸ˆìœµì •ì±…
            'é‡‘åˆ©',          # ê¸ˆë¦¬
            'ç‚ºæ›¿',          # í™˜ìœ¨
            'ç‰©ä¾¡',          # ë¬¼ê°€
            'å††',            # ì—”í™”
            'ä»‹å…¥',          # ê°œì… (ì™¸í™˜ì‹œì¥ ê°œì…)
            'æ±ºå®šä¼šåˆ',      # ì •ì±…ê²°ì •íšŒì˜
            'ç·©å’Œ',          # ì™„í™” (ëˆ í’€ê¸°)
            'å‡ºå£',          # ì¶œêµ¬ (ê¸´ì¶• ì „í™˜)

            # --- í•µì‹¬ ì§ì±… ---
            'ç·è£',          # ì´ì¬ (BOJ ìˆ˜ì¥)
            'å‰¯ç·è£',        # ë¶€ì´ì¬
            'å¯©è­°å§”å“¡',      # ì‹¬ì˜ìœ„ì› (BOJ ê¸ˆí†µìœ„ì›)
            'è²¡å‹™ç›¸',        # ì¬ë¬´ìƒ (ì¬ë¬´ë¶€ ì¥ê´€)
            'è²¡å‹™å®˜',        # ì¬ë¬´ê´€ (ì‹¤ë¬´ ì±…ì„ì)
            'é¦–ç›¸',          # ìˆ˜ìƒ (ì´ë¦¬)
            'é–£åƒš',          # ê°ë£Œ (ì¥ê´€ê¸‰ ì¸ì‚¬)

            # --- í•µì‹¬ ê²½ì œ ì§€í‘œ ---
            'GDP',          # êµ­ë‚´ì´ìƒì‚°
            'CPI',          # ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜
            'çŸ­è¦³',          # ë‹¨ì¹¸ì§€í‘œ (ê¸°ì—…ê²½ê¸°ì‹¤ì‚¬ì§€í‘œ)
            'è²¿æ˜“åæ”¯',      # ë¬´ì—­ìˆ˜ì§€ (ìˆ˜ì¶œì… ìƒí™©)
            'å¤±æ¥­ç‡',        # ì‹¤ì—…ë¥ 
            'è³ƒé‡‘',          # ì„ê¸ˆ (BOJê°€ ìµœê·¼ ê°€ì¥ ê°•ì¡°í•¨)
            'å®Ÿè³ªè³ƒé‡‘',      # ì‹¤ì§ˆì„ê¸ˆ (ì´ê²Œ ì˜¬ë¼ì•¼ ê¸ˆë¦¬ ì˜¬ë¦¼)
            'å®¶è¨ˆèª¿æŸ»',      # ê°€ê³„ì¡°ì‚¬ (ì†Œë¹„ì ì§€ì¶œ)
            'æ©Ÿæ¢°å—æ³¨',      # ê¸°ê³„ìˆ˜ì£¼ (ê¸°ì—… íˆ¬ì ì§€í‘œ)
            'æ™¯æ°—å‹•å‘æŒ‡æ•°',   # ê²½ê¸°ë™í–¥ì§€ìˆ˜

            # --- í•µì‹¬ ì•¡ì…˜ ë° ìƒíƒœ ---
            'åˆ©ä¸Šã’',        # ê¸ˆë¦¬ ì¸ìƒ
            'åˆ©ä¸‹ã’',        # ê¸ˆë¦¬ ì¸í•˜
            'æ®ãˆç½®ã',      # ê¸ˆë¦¬ ë™ê²°
            'ä¿®æ­£',          # ì •ì±… ìˆ˜ì •
            'ç‚¹æ¤œ',          # ì •ì±… ì ê²€
            'å½“é¢',          # ë‹¹ë¶„ê°„ (ì •ì±… ìœ ì§€ ì‹œ ìì£¼ ì”€)
            'è¸ã¿è¾¼ã‚€'       # (ì •ì±…ì—) ë°œì„ ë“¤ì´ë‹¤, ë‹¨í–‰í•˜ë‹¤
        ]
        
        self.db_path = "macro_intelligence.db"
        self._init_db()

    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute('''CREATE TABLE IF NOT EXISTS news 
                       (link TEXT PRIMARY KEY, title TEXT, source TEXT, 
                        country TEXT, date TEXT, collected_at TEXT)''')
        conn.commit()
        conn.close()

    def _standardize_date(self, date_str):
        """ì§€ì €ë¶„í•œ ë‚ ì§œë¥¼ 'YYYY-MM-DD HH:MM:SS'ë¡œ ì„¸íƒ"""
        if not date_str:
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        try:
            dt = parser.parse(date_str)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            return date_str.replace('T', ' ').replace('Z', '')[:19]

    def _is_new(self, link):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute("SELECT link FROM news WHERE link=?", (link,))
        exists = cur.fetchone()
        conn.close()
        return exists is None

    def _save_to_db(self, item, country):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        try:
            cur.execute("INSERT OR IGNORE INTO news (link, title, source, country, date, collected_at) VALUES (?, ?, ?, ?, ?, ?)",
                        (item['link'], item['title'], item['source'], country, item['date'], datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            conn.commit()
        except sqlite3.IntegrityError:
            pass
        finally:
            conn.close()

    def _get_soup(self, url, is_xml=False):
        try:
            res = requests.get(url, headers=self.headers, verify=False, timeout=15)
            if res.status_code == 200:
                features = "xml" if is_xml else "html.parser"
                return BeautifulSoup(res.content, features)
        except Exception as e:
            print(f"ğŸ“¡ ì ‘ì† ì—ëŸ¬ ({url}): {e}")
        return None

    def scout_yahoo(self):
        """ì•¼í›„ íŒŒì´ë‚¸ìŠ¤: í•„í„° ì—†ì´ ì „ì²´ ìˆ˜ì§‘"""
        sources = [
            {"name": "Yahoo_Index", "url": "https://finance.yahoo.com/news/rssindex"},
            {"name": "Yahoo_Stocks", "url": "https://finance.yahoo.com/rss/stocks"}
        ]
        results = []
        for src in sources:
            soup = self._get_soup(src['url'], is_xml=True)
            if not soup: continue
            items = soup.find_all('item')
            for item in items:
                title = item.title.text.strip()
                link = item.link.text.split('?')[0]
                pub_date = item.pubDate.text if item.pubDate else ""
                results.append({
                    "source": src['name'],
                    "date": self._standardize_date(pub_date),
                    "title": title,
                    "link": link
                })
        return results

    def scout_news_jp(self):
        """ì¼ë³¸ ë‰´ìŠ¤: ì‚¬ë ¹ê´€ë‹˜ì˜ 35ê°œ í‚¤ì›Œë“œë¡œ ì •ë°€ í•„í„°ë§"""
        sources = [
            {"name": "Jiji", "url": "https://www.jiji.com/jc/c?g=eco"},
            {"name": "Reuters_JP", "url": "https://jp.reuters.com/markets/japan/"}
        ]
        results = []
        for src in sources:
            soup = self._get_soup(src['url'])
            if not soup: continue
            links = soup.find_all('a', href=True)
            for link in links:
                title = link.get_text().strip()
                if len(title) > 15 and any(kw in title for kw in self.jp_keywords):
                    href = link['href']
                    full_link = href if href.startswith('http') else (
                        "https://www.jiji.com" + href if src['name'] == "Jiji" else "https://jp.reuters.com" + href
                    )
                    results.append({
                        "source": src['name'],
                        "date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "title": title,
                        "link": full_link
                    })
        return results

    def run_all_stations(self):
        """ì „ ì§€ì—­ í†µí•© ì •ì°° ì‹¤í–‰"""
        print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] ì „ ì§€ì—­ í†µí•© ì •ì°° ê°œì‹œ...")
        
        # 1. ë¯¸êµ­/ê¸€ë¡œë²Œ (Yahoo - ì „ì²´ ìˆ˜ì§‘)
        y_count = 0
        for item in self.scout_yahoo():
            if self._is_new(item['link']):
                self._save_to_db(item, country="USA/Global")
                y_count += 1
        
        # 2. ì¼ë³¸ (Jiji, Reuters - í‚¤ì›Œë“œ í•„í„°ë§)
        j_count = 0
        for item in self.scout_news_jp():
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                j_count += 1
        
        print(f"ğŸ ì‘ì „ ì¢…ë£Œ: ì‹ ê·œ ì²©ë³´ {y_count + j_count}ê±´ í™•ë³´ (USA: {y_count}, Japan: {j_count})")

if __name__ == "__main__":
    scouter = MacroScouter()
    scouter.run_all_stations()
