import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
from datetime import datetime
import time


class MacroScouter:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        # í•„í„° í‚¤ì›Œë“œ
        self.jp_keywords = ['æ—¥éŠ€', 'é‡‘ë¦¬', 'ç‚ºæ›¿', 'ç‰©ä¾¡',
                            'æ™¯æ°£', 'å††', 'ì´ì¬', 'æ¤ç”°', 'è³ƒä¸Šã’', 'å›½å†…']
        self.db_path = "macro_intelligence.db"
        self._init_db()

    def _init_db(self):
        """DB ì´ˆê¸°í™” (country ì»¬ëŸ¼ í¬í•¨)"""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute('''CREATE TABLE IF NOT EXISTS news 
                       (link TEXT PRIMARY KEY, title TEXT, source TEXT, 
                        country TEXT, date TEXT, collected_at TEXT)''')
        conn.commit()
        conn.close()

    def _is_new(self, link):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        cur.execute("SELECT link FROM news WHERE link=?", (link,))
        exists = cur.fetchone()
        conn.close()
        return exists is None

    def _save_to_db(self, item, country="Japan"):
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        try:
            cur.execute("INSERT INTO news (link, title, source, country, date, collected_at) VALUES (?, ?, ?, ?, ?, ?)",
                        (item['link'], item['title'], item['source'], country, item['date'], datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            conn.commit()
        except sqlite3.IntegrityError:
            pass
        finally:
            conn.close()

    def _get_soup(self, url):
        try:
            res = requests.get(url, headers=self.headers, timeout=15)
            res.encoding = 'utf-8'
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
        except Exception as e:
            print(f"Connection Error ({url}): {e}")
        return None

    def scout_boj_universal(self, year=None):
        """[ì„±ê³µí–ˆë˜ ë¡œì§] ë§í¬ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ BOJ ìë£Œë¥¼ ë‚šì•„ì±•ë‹ˆë‹¤."""
        if year:
            url = f"https://www.boj.or.jp/about/press/koen_{year}/index.htm"
            source_name = f"BOJ_{year}"
        else:
            url = "https://www.boj.or.jp/about/press/index.htm"
            source_name = "BOJ_Latest"

        soup = self._get_soup(url)
        results = []
        if not soup:
            return results

        # ì•„ê¹Œ ì„±ê³µí–ˆë˜ 'ëª¨ë“  ë§í¬ ë’¤ì§€ê¸°' ì „ëµ
        links = soup.find_all('a', href=True)
        for a in links:
            href = a['href']
            title = a.get_text().strip()

            # BOJ ê°•ì—°ë¬¸/ë³´ë„ìë£Œ íŠ¹ìœ ì˜ ë§í¬ íŒ¨í„´ (/ko ë˜ëŠ” koen)
            if ('/ko' in href or 'koen' in href) and len(title) > 10:
                full_link = "https://www.boj.or.jp" + \
                    href if href.startswith('/') else href

                # ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                date = f"{year}-XX" if year else "Latest"
                try:
                    # í‘œ êµ¬ì¡°ì¸ ê²½ìš° ì• ì¹¸ì˜ ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
                    date = a.find_parent('tr').find('td').get_text().strip()
                except:
                    pass

                results.append({
                    "source": source_name,
                    "date": date,
                    "title": title,
                    "link": full_link
                })
        return results

    def scout_news_jp(self):
        """ì§€êµ¬í†µì‹  ë° ë¡œì´í„° ì •ì°°"""
        sources = [
            {"name": "Jiji", "url": "https://www.jiji.com/jc/c?g=eco"},
            {"name": "Reuters", "url": "https://jp.reuters.com/markets/japan/"}
        ]
        results = []
        for src in sources:
            soup = self._get_soup(src['url'])
            if not soup:
                continue

            links = soup.find_all('a', href=True)
            for link in links:
                title = link.get_text().strip()
                # í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
                if len(title) > 15 and any(kw in title for kw in self.jp_keywords):
                    href = link['href']
                    full_link = href if href.startswith('http') else (
                        "https://www.jiji.com" +
                        href if src['name'] == "Jiji" else "https://jp.reuters.com" + href
                    )
                    results.append({
                        "source": src['name'], "date": "Today", "title": title, "link": full_link
                    })
        return results

    def run_all_jp(self):
        """ì¼ë³¸ ì •ì°° í†µí•© ì‹¤í–‰"""
        print("ğŸ›°ï¸ ì¼ë³¸ ë§¤í¬ë¡œ í†µí•© ì •ì°° ê°œì‹œ...")
        new_items = []

        # 1. BOJ ìµœì‹ 
        for item in self.scout_boj_universal():
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                new_items.append(item)

        # 2. ë‰´ìŠ¤ (ì§€ì, ë¡œì´í„°)
        for item in self.scout_news_jp():
            if self._is_new(item['link']):
                self._save_to_db(item, country="Japan")
                new_items.append(item)
        return new_items


if __name__ == "__main__":
    scouter = MacroScouter()

    # [1] ê³¼ê±° ë°ì´í„° ì±„ìš°ê¸° (2021~2025)
    print("â³ ê³¼ê±° ì•„ì¹´ì´ë¸Œ ì±„ìš°ëŠ” ì¤‘...")
    for y in range(2021, 2026):
        historical = scouter.scout_boj_universal(year=y)
        count = 0
        for item in historical:
            if scouter._is_new(item['link']):
                scouter._save_to_db(item, country="Japan")
                count += 1
        print(f"âœ… {y}ë…„ë„ ì™„ë£Œ: {count}ê±´ ì €ì¥")

    # [2] ì˜¤ëŠ˜ì ìµœì‹  ì •ì°°
    new_data = scouter.run_all_jp()
    if new_data:
        print(f"ğŸ”¥ ì‹ ê·œ ë°ì´í„° {len(new_data)}ê±´ ë°œê²¬!")
    else:
        print("â„¹ï¸ ìƒˆë¡œ ì—…ë°ì´íŠ¸ëœ ì†Œì‹ ì—†ìŒ.")
